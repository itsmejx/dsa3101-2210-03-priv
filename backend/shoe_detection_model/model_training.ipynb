{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setup Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKSPACE_PATH = 'Tensorflow/workspace'\n",
    "SCRIPTS_PATH = 'Tensorflow/scripts'\n",
    "APIMODEL_PATH = 'Tensorflow/models'\n",
    "ANNOTATION_PATH = WORKSPACE_PATH+'/annotations'\n",
    "IMAGE_PATH = WORKSPACE_PATH+'/images'\n",
    "MODEL_PATH = WORKSPACE_PATH+'/models'\n",
    "PRETRAINED_MODEL_PATH = WORKSPACE_PATH+'/pre-trained-models'\n",
    "CONFIG_PATH = MODEL_PATH+'/my_ssd_mobnet/pipeline.config'\n",
    "CHECKPOINT_PATH = MODEL_PATH+'/my_ssd_mobnet/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Create Label Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [{'name':'shoe', 'id':1}]\n",
    "\n",
    "with open(ANNOTATION_PATH + '\\label_map.pbtxt', 'w') as f:\n",
    "    for label in labels:\n",
    "        f.write('item { \\n')\n",
    "        f.write('\\tname:\\'{}\\'\\n'.format(label['name']))\n",
    "        f.write('\\tid:{}\\n'.format(label['id']))\n",
    "        f.write('}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create TF records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created the TFRecord file: Tensorflow/workspace/annotations/train.record\n",
      "Successfully created the TFRecord file: Tensorflow/workspace/annotations/test.record\n"
     ]
    }
   ],
   "source": [
    "!python {SCRIPTS_PATH + '/generate_tfrecord.py'} -x {IMAGE_PATH + '/train'} -l {ANNOTATION_PATH + '/label_map.pbtxt'} -o {ANNOTATION_PATH + '/train.record'}\n",
    "!python {SCRIPTS_PATH + '/generate_tfrecord.py'} -x{IMAGE_PATH + '/test'} -l {ANNOTATION_PATH + '/label_map.pbtxt'} -o {ANNOTATION_PATH + '/test.record'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Download TF Models Pretrained Models from Tensorflow Model Zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'models'...\n",
      "error: unable to create file official/projects/backbone_reuse/configs/experiments/faster_rcnn/fastrcnn_resnet101_nasfpn_cascade_600epochs.yaml: Filename too long\n",
      "error: unable to create file official/projects/backbone_reuse/configs/experiments/faster_rcnn/fastrcnn_resnet101_nasfpn_cascade_72epochs.yaml: Filename too long\n",
      "Updating files:  21% (676/3188)\n",
      "Updating files:  22% (702/3188)\n",
      "Updating files:  23% (734/3188)\n",
      "Updating files:  24% (766/3188)\n",
      "Updating files:  25% (797/3188)\n",
      "Updating files:  26% (829/3188)\n",
      "Updating files:  27% (861/3188)\n",
      "Updating files:  28% (893/3188)\n",
      "error: unable to create file official/projects/qat/vision/configs/experiments/image_classification/imagenet_mobilenetv2_qat_gpu_batch256.yaml: Filename too long\n",
      "error: unable to create file official/projects/qat/vision/configs/experiments/image_classification/imagenet_mobilenetv2_qat_gpu_batch512.yaml: Filename too long\n",
      "error: unable to create file official/projects/qat/vision/configs/experiments/semantic_segmentation/deeplabv3_mobilenetv2_pascal_qat_gpu.yaml: Filename too long\n",
      "Updating files:  29% (925/3188)\n",
      "error: unable to create file official/projects/qat/vision/configs/experiments/semantic_segmentation/deeplabv3_mobilenetv2_pascal_qat_tpu.yaml: Filename too long\n",
      "error: unable to create file official/projects/qat/vision/configs/experiments/semantic_segmentation/deeplabv3plus_mobilenetv2_cityscapes_qat_tpu.yaml: Filename too long\n",
      "Updating files:  30% (957/3188)\n",
      "Updating files:  31% (989/3188)\n",
      "Updating files:  32% (1021/3188)\n",
      "Updating files:  33% (1053/3188)\n",
      "Updating files:  34% (1084/3188)\n",
      "Updating files:  35% (1116/3188)\n",
      "Updating files:  36% (1148/3188)\n",
      "error: unable to create file official/projects/waste_identification_ml/pre_processing/config/sample_images/ffdeb4cd-43ba-4ca0-a1e6-aa5824005f44.jpg: Filename too long\n",
      "error: unable to create file official/projects/waste_identification_ml/pre_processing/config/sample_json/ffdeb4cd-43ba-4ca0-a1e6-aa5824005f44.json: Filename too long\n",
      "Updating files:  36% (1177/3188)\n",
      "Updating files:  37% (1180/3188)\n",
      "Updating files:  38% (1212/3188)\n",
      "Updating files:  39% (1244/3188)\n",
      "Updating files:  40% (1276/3188)\n",
      "Updating files:  41% (1308/3188)\n",
      "Updating files:  42% (1339/3188)\n",
      "Updating files:  43% (1371/3188)\n",
      "Updating files:  44% (1403/3188)\n",
      "Updating files:  45% (1435/3188)\n",
      "Updating files:  46% (1467/3188)\n",
      "Updating files:  47% (1499/3188)\n",
      "Updating files:  48% (1531/3188)\n",
      "Updating files:  49% (1563/3188)\n",
      "Updating files:  50% (1594/3188)\n",
      "Updating files:  51% (1626/3188)\n",
      "Updating files:  52% (1658/3188)\n",
      "Updating files:  53% (1690/3188)\n",
      "Updating files:  54% (1722/3188)\n",
      "Updating files:  55% (1754/3188)\n",
      "Updating files:  56% (1786/3188)\n",
      "Updating files:  56% (1798/3188)\n",
      "Updating files:  57% (1818/3188)\n",
      "Updating files:  58% (1850/3188)\n",
      "Updating files:  59% (1881/3188)\n",
      "Updating files:  60% (1913/3188)\n",
      "Updating files:  61% (1945/3188)\n",
      "Updating files:  62% (1977/3188)\n",
      "Updating files:  63% (2009/3188)\n",
      "Updating files:  64% (2041/3188)\n",
      "Updating files:  65% (2073/3188)\n",
      "Updating files:  66% (2105/3188)\n",
      "Updating files:  67% (2136/3188)\n",
      "Updating files:  68% (2168/3188)\n",
      "Updating files:  69% (2200/3188)\n",
      "Updating files:  70% (2232/3188)\n",
      "Updating files:  71% (2264/3188)\n",
      "Updating files:  72% (2296/3188)\n",
      "Updating files:  73% (2328/3188)\n",
      "Updating files:  74% (2360/3188)\n",
      "Updating files:  75% (2391/3188)\n",
      "Updating files:  76% (2423/3188)\n",
      "Updating files:  77% (2455/3188)\n",
      "Updating files:  78% (2487/3188)\n",
      "Updating files:  79% (2519/3188)\n",
      "Updating files:  80% (2551/3188)\n",
      "Updating files:  81% (2583/3188)\n",
      "Updating files:  82% (2615/3188)\n",
      "Updating files:  83% (2647/3188)\n",
      "Updating files:  84% (2678/3188)\n",
      "Updating files:  85% (2710/3188)\n",
      "error: unable to create file research/object_detection/samples/configs/ssd_mobilenet_v2_fpnlite_quantized_shared_box_predictor_256x256_depthmultiplier_75_coco14_sync.config: Filename too long\n",
      "error: unable to create file research/object_detection/samples/configs/ssd_mobilenet_v2_mnasfpn_shared_box_predictor_320x320_coco_sync.config: Filename too long\n",
      "Updating files:  86% (2742/3188)\n",
      "Updating files:  87% (2774/3188)\n",
      "Updating files:  88% (2806/3188)\n",
      "Updating files:  89% (2838/3188)\n",
      "Updating files:  90% (2870/3188)\n",
      "Updating files:  91% (2902/3188)\n",
      "Updating files:  92% (2933/3188)\n",
      "Updating files:  93% (2965/3188)\n",
      "Updating files:  94% (2997/3188)\n",
      "Updating files:  95% (3029/3188)\n",
      "Updating files:  96% (3061/3188)\n",
      "Updating files:  96% (3078/3188)\n",
      "Updating files:  97% (3093/3188)\n",
      "Updating files:  98% (3125/3188)\n",
      "Updating files:  99% (3157/3188)\n",
      "Updating files: 100% (3188/3188)\n",
      "Updating files: 100% (3188/3188), done.\n",
      "fatal: unable to checkout working tree\n",
      "warning: Clone succeeded, but checkout failed.\n",
      "You can inspect what was checked out with 'git status'\n",
      "and retry with 'git restore --source=HEAD :/'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cd Tensorflow && git clone https://github.com/tensorflow/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wget.download('http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz')\n",
    "#!mv ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz {PRETRAINED_MODEL_PATH}\n",
    "#!cd {PRETRAINED_MODEL_PATH} && tar -zxvf ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Copy Model Config to Training Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOM_MODEL_NAME = 'my_ssd_mobnet' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file Tensorflow\\workspace\\models\\my_ssd_mobnet already exists.\n",
      "'cp' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!mkdir {'Tensorflow\\workspace\\models\\\\'+CUSTOM_MODEL_NAME}\n",
    "!cp {PRETRAINED_MODEL_PATH+'/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/pipeline.config'} {MODEL_PATH+'/'+CUSTOM_MODEL_NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Update Config For Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.protos import pipeline_pb2\n",
    "from google.protobuf import text_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = MODEL_PATH+'/'+CUSTOM_MODEL_NAME+'/pipeline.config'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = config_util.get_configs_from_pipeline_file(CONFIG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': ssd {\n",
       "   num_classes: 1\n",
       "   image_resizer {\n",
       "     fixed_shape_resizer {\n",
       "       height: 320\n",
       "       width: 320\n",
       "     }\n",
       "   }\n",
       "   feature_extractor {\n",
       "     type: \"ssd_mobilenet_v2_fpn_keras\"\n",
       "     depth_multiplier: 1.0\n",
       "     min_depth: 16\n",
       "     conv_hyperparams {\n",
       "       regularizer {\n",
       "         l2_regularizer {\n",
       "           weight: 3.9999998989515007e-05\n",
       "         }\n",
       "       }\n",
       "       initializer {\n",
       "         random_normal_initializer {\n",
       "           mean: 0.0\n",
       "           stddev: 0.009999999776482582\n",
       "         }\n",
       "       }\n",
       "       activation: RELU_6\n",
       "       batch_norm {\n",
       "         decay: 0.996999979019165\n",
       "         scale: true\n",
       "         epsilon: 0.0010000000474974513\n",
       "       }\n",
       "     }\n",
       "     use_depthwise: true\n",
       "     override_base_feature_extractor_hyperparams: true\n",
       "     fpn {\n",
       "       min_level: 3\n",
       "       max_level: 7\n",
       "       additional_layer_depth: 128\n",
       "     }\n",
       "   }\n",
       "   box_coder {\n",
       "     faster_rcnn_box_coder {\n",
       "       y_scale: 10.0\n",
       "       x_scale: 10.0\n",
       "       height_scale: 5.0\n",
       "       width_scale: 5.0\n",
       "     }\n",
       "   }\n",
       "   matcher {\n",
       "     argmax_matcher {\n",
       "       matched_threshold: 0.5\n",
       "       unmatched_threshold: 0.5\n",
       "       ignore_thresholds: false\n",
       "       negatives_lower_than_unmatched: true\n",
       "       force_match_for_each_row: true\n",
       "       use_matmul_gather: true\n",
       "     }\n",
       "   }\n",
       "   similarity_calculator {\n",
       "     iou_similarity {\n",
       "     }\n",
       "   }\n",
       "   box_predictor {\n",
       "     weight_shared_convolutional_box_predictor {\n",
       "       conv_hyperparams {\n",
       "         regularizer {\n",
       "           l2_regularizer {\n",
       "             weight: 3.9999998989515007e-05\n",
       "           }\n",
       "         }\n",
       "         initializer {\n",
       "           random_normal_initializer {\n",
       "             mean: 0.0\n",
       "             stddev: 0.009999999776482582\n",
       "           }\n",
       "         }\n",
       "         activation: RELU_6\n",
       "         batch_norm {\n",
       "           decay: 0.996999979019165\n",
       "           scale: true\n",
       "           epsilon: 0.0010000000474974513\n",
       "         }\n",
       "       }\n",
       "       depth: 128\n",
       "       num_layers_before_predictor: 4\n",
       "       kernel_size: 3\n",
       "       class_prediction_bias_init: -4.599999904632568\n",
       "       share_prediction_tower: true\n",
       "       use_depthwise: true\n",
       "     }\n",
       "   }\n",
       "   anchor_generator {\n",
       "     multiscale_anchor_generator {\n",
       "       min_level: 3\n",
       "       max_level: 7\n",
       "       anchor_scale: 4.0\n",
       "       aspect_ratios: 1.0\n",
       "       aspect_ratios: 2.0\n",
       "       aspect_ratios: 0.5\n",
       "       scales_per_octave: 2\n",
       "     }\n",
       "   }\n",
       "   post_processing {\n",
       "     batch_non_max_suppression {\n",
       "       score_threshold: 9.99999993922529e-09\n",
       "       iou_threshold: 0.6000000238418579\n",
       "       max_detections_per_class: 100\n",
       "       max_total_detections: 100\n",
       "       use_static_shapes: false\n",
       "     }\n",
       "     score_converter: SIGMOID\n",
       "   }\n",
       "   normalize_loss_by_num_matches: true\n",
       "   loss {\n",
       "     localization_loss {\n",
       "       weighted_smooth_l1 {\n",
       "       }\n",
       "     }\n",
       "     classification_loss {\n",
       "       weighted_sigmoid_focal {\n",
       "         gamma: 2.0\n",
       "         alpha: 0.25\n",
       "       }\n",
       "     }\n",
       "     classification_weight: 1.0\n",
       "     localization_weight: 1.0\n",
       "   }\n",
       "   encode_background_as_zeros: true\n",
       "   normalize_loc_loss_by_codesize: true\n",
       "   inplace_batchnorm_update: true\n",
       "   freeze_batchnorm: false\n",
       " },\n",
       " 'train_config': batch_size: 4\n",
       " data_augmentation_options {\n",
       "   random_horizontal_flip {\n",
       "   }\n",
       " }\n",
       " data_augmentation_options {\n",
       "   random_crop_image {\n",
       "     min_object_covered: 0.0\n",
       "     min_aspect_ratio: 0.75\n",
       "     max_aspect_ratio: 3.0\n",
       "     min_area: 0.75\n",
       "     max_area: 1.0\n",
       "     overlap_thresh: 0.0\n",
       "   }\n",
       " }\n",
       " sync_replicas: true\n",
       " optimizer {\n",
       "   momentum_optimizer {\n",
       "     learning_rate {\n",
       "       cosine_decay_learning_rate {\n",
       "         learning_rate_base: 0.07999999821186066\n",
       "         total_steps: 50000\n",
       "         warmup_learning_rate: 0.026666000485420227\n",
       "         warmup_steps: 1000\n",
       "       }\n",
       "     }\n",
       "     momentum_optimizer_value: 0.8999999761581421\n",
       "   }\n",
       "   use_moving_average: false\n",
       " }\n",
       " fine_tune_checkpoint: \"Tensorflow/workspace/pre-trained-models/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/checkpoint/ckpt-0\"\n",
       " num_steps: 50000\n",
       " startup_delay_steps: 0.0\n",
       " replicas_to_aggregate: 8\n",
       " max_number_of_boxes: 100\n",
       " unpad_groundtruth_tensors: false\n",
       " fine_tune_checkpoint_type: \"detection\"\n",
       " fine_tune_checkpoint_version: V2,\n",
       " 'train_input_config': label_map_path: \"Tensorflow/workspace/annotations/label_map.pbtxt\"\n",
       " tf_record_input_reader {\n",
       "   input_path: \"Tensorflow/workspace/annotations/train.record\"\n",
       " },\n",
       " 'eval_config': metrics_set: \"coco_detection_metrics\"\n",
       " use_moving_averages: false,\n",
       " 'eval_input_configs': [label_map_path: \"Tensorflow/workspace/annotations/label_map.pbtxt\"\n",
       " shuffle: false\n",
       " num_epochs: 1\n",
       " tf_record_input_reader {\n",
       "   input_path: \"Tensorflow/workspace/annotations/test.record\"\n",
       " }\n",
       " ],\n",
       " 'eval_input_config': label_map_path: \"Tensorflow/workspace/annotations/label_map.pbtxt\"\n",
       " shuffle: false\n",
       " num_epochs: 1\n",
       " tf_record_input_reader {\n",
       "   input_path: \"Tensorflow/workspace/annotations/test.record\"\n",
       " }}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n",
    "with tf.io.gfile.GFile(CONFIG_PATH, \"r\") as f:                                                                                                                                                                                                                     \n",
    "    proto_str = f.read()                                                                                                                                                                                                                                          \n",
    "    text_format.Merge(proto_str, pipeline_config)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_config.model.ssd.num_classes = 1\n",
    "pipeline_config.train_config.batch_size = 4\n",
    "pipeline_config.train_config.fine_tune_checkpoint = PRETRAINED_MODEL_PATH+'/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/checkpoint/ckpt-0'\n",
    "pipeline_config.train_config.fine_tune_checkpoint_type = \"detection\"\n",
    "pipeline_config.train_input_reader.label_map_path= ANNOTATION_PATH + '/label_map.pbtxt'\n",
    "pipeline_config.train_input_reader.tf_record_input_reader.input_path[:] = [ANNOTATION_PATH + '/train.record']\n",
    "pipeline_config.eval_input_reader[0].label_map_path = ANNOTATION_PATH + '/label_map.pbtxt'\n",
    "pipeline_config.eval_input_reader[0].tf_record_input_reader.input_path[:] = [ANNOTATION_PATH + '/test.record']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_text = text_format.MessageToString(pipeline_config)                                                                                                                                                                                                        \n",
    "with tf.io.gfile.GFile(CONFIG_PATH, \"wb\") as f:                                                                                                                                                                                                                     \n",
    "    f.write(config_text)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python Tensorflow/models/research/object_detection/model_main_tf2.py --model_dir=Tensorflow/workspace/models/my_ssd_mobnet --pipeline_config_path=Tensorflow/workspace/models/my_ssd_mobnet/pipeline.config --num_train_steps=5000\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"python {}/research/object_detection/model_main_tf2.py --model_dir={}/{} --pipeline_config_path={}/{}/pipeline.config --num_train_steps=5000\"\"\".format(APIMODEL_PATH, MODEL_PATH,CUSTOM_MODEL_NAME,MODEL_PATH,CUSTOM_MODEL_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Load Train Model From Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "from object_detection.builders import model_builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pipeline config and build a detection model\n",
    "configs = config_util.get_configs_from_pipeline_file(CONFIG_PATH)\n",
    "detection_model = model_builder.build(model_config=configs['model'], is_training=False)\n",
    "\n",
    "# Restore checkpoint\n",
    "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "ckpt.restore(os.path.join(CHECKPOINT_PATH, 'ckpt-')).expect_partial()\n",
    "\n",
    "@tf.function\n",
    "def detect_fn(image):\n",
    "    image, shapes = detection_model.preprocess(image)\n",
    "    prediction_dict = detection_model.predict(image, shapes)\n",
    "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
    "    return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_index = label_map_util.create_category_index_from_labelmap(ANNOTATION_PATH+'/label_map.pbtxt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from object_detection.utils import config_util\n",
    "import os\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "from object_detection.builders import model_builder\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_as_raw_output(path2images,\n",
    "                            box_th = 0.25,\n",
    "                            nms_th = 0.5,\n",
    "                            to_file = False,\n",
    "                            data = None,\n",
    "                            path2dir = False):\n",
    "    \"\"\"\n",
    "    Function that performs inference and return filtered predictions\n",
    "    \n",
    "    Args:\n",
    "      path2images: an array with pathes to images\n",
    "      box_th: (float) value that defines threshold for model prediction. Consider 0.25 as a value.\n",
    "      nms_th: (float) value that defines threshold for non-maximum suppression. Consider 0.5 as a value.\n",
    "      to_file: (boolean). When passed as True => results are saved into a file. Writing format is\n",
    "      path2image + (x1abs, y1abs, x2abs, y2abs, score, conf) for box in boxes\n",
    "      data: (str) name of the dataset you passed in (e.g. test/validation)\n",
    "      path2dir: (str). Should be passed if path2images has only basenames. If full pathes provided => set False.\n",
    "      \n",
    "    Returns:\n",
    "      detections (dict): filtered predictions that model made\n",
    "    \"\"\"\n",
    "    \n",
    "    for image_path in path2images:\n",
    "        if path2dir: # if a path to a directory where images are stored was passed in\n",
    "            image_path = os.path.join(path2dir, image_path.strip())\n",
    "            \n",
    "        image_np = load_image_into_numpy_array(image_path)\n",
    "\n",
    "        input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "        detections = detect_fn(input_tensor)\n",
    "        \n",
    "        # checking how many detections we got\n",
    "        num_detections = int(detections.pop('num_detections'))\n",
    "        \n",
    "        # filtering out detection in order to get only the one that are indeed detections\n",
    "        detections = {key: value[0, :num_detections].numpy() for key, value in detections.items()}\n",
    "        \n",
    "        # detection_classes should be ints.\n",
    "        label_id_offset = 1\n",
    "        detections['detection_classes'] = detections['detection_classes'].astype(np.int64)+label_id_offset\n",
    "        \n",
    "        # defining what we need from the resulting detection dict that we got from model output\n",
    "        key_of_interest = ['detection_classes', 'detection_boxes', 'detection_scores']\n",
    "        \n",
    "        # filtering out detection dict in order to get only boxes, classes and scores\n",
    "        detections = {key: value for key, value in detections.items() if key in key_of_interest}\n",
    "        \n",
    "        if box_th: # filtering detection if a confidence threshold for boxes was given as a parameter\n",
    "            for key in key_of_interest:\n",
    "                scores = detections['detection_scores']\n",
    "                current_array = detections[key]\n",
    "                filtered_current_array = current_array[scores > box_th]\n",
    "                detections[key] = filtered_current_array\n",
    "        \n",
    "        if nms_th: # filtering rectangles if nms threshold was passed in as a parameter\n",
    "            # creating a zip object that will contain model output info as\n",
    "            output_info = list(zip(detections['detection_boxes'],\n",
    "                                   detections['detection_scores'],\n",
    "                                   detections['detection_classes']\n",
    "                                  )\n",
    "                              )\n",
    "            boxes, scores, classes = nms(output_info)\n",
    "            \n",
    "            detections['detection_boxes'] = boxes # format: [y1, x1, y2, x2]\n",
    "            detections['detection_scores'] = scores\n",
    "            detections['detection_classes'] = classes\n",
    "            \n",
    "        if to_file and data: # if saving to txt file was requested\n",
    "\n",
    "            image_h, image_w, _ = image_np.shape\n",
    "            file_name = f'pred_result_{data}.txt'\n",
    "            \n",
    "            line2write = list()\n",
    "            line2write.append(os.path.basename(image_path))\n",
    "            \n",
    "            with open(file_name, 'a+') as text_file:\n",
    "                # iterating over boxes\n",
    "                for b, s, c in zip(boxes, scores, classes):\n",
    "                    \n",
    "                    y1abs, x1abs = b[0] * image_h, b[1] * image_w\n",
    "                    y2abs, x2abs = b[2] * image_h, b[3] * image_w\n",
    "                    \n",
    "                    list2append = [x1abs, y1abs, x2abs, y2abs, s, c]\n",
    "                    line2append = ','.join([str(item) for item in list2append])\n",
    "                    \n",
    "                    line2write.append(line2append)\n",
    "                \n",
    "                line2write = ' '.join(line2write)\n",
    "                text_file.write(line2write + os.linesep)\n",
    "        \n",
    "        return detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_into_numpy_array(path):\n",
    "    \"\"\"Load an image from file into a numpy array.\n",
    "\n",
    "    Puts image into numpy array to feed into tensorflow graph.\n",
    "    Note that by convention we put it into a numpy array with shape\n",
    "    (height, width, channels), where channels=3 for RGB.\n",
    "\n",
    "    Args:\n",
    "      path: the file path to the image\n",
    "\n",
    "    Returns:\n",
    "      numpy array with shape (img_height, img_width, 3)\n",
    "    \"\"\"\n",
    "    \n",
    "    return np.array(Image.open(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nms(rects, thd=0.5):\n",
    "    \"\"\"\n",
    "    Filter rectangles\n",
    "    rects is array of oblects ([x1,y1,x2,y2], confidence, class)\n",
    "    thd - intersection threshold (intersection divides min square of rectange)\n",
    "    \"\"\"\n",
    "    out = []\n",
    "\n",
    "    remove = [False] * len(rects)\n",
    "\n",
    "    for i in range(0, len(rects) - 1):\n",
    "        if remove[i]:\n",
    "            continue\n",
    "        inter = [0.0] * len(rects)\n",
    "        for j in range(i, len(rects)):\n",
    "            if remove[j]:\n",
    "                continue\n",
    "            inter[j] = intersection(rects[i][0], rects[j][0]) / min(square(rects[i][0]), square(rects[j][0]))\n",
    "\n",
    "        max_prob = 0.0\n",
    "        max_idx = 0\n",
    "        for k in range(i, len(rects)):\n",
    "            if inter[k] >= thd:\n",
    "                if rects[k][1] > max_prob:\n",
    "                    max_prob = rects[k][1]\n",
    "                    max_idx = k\n",
    "\n",
    "        for k in range(i, len(rects)):\n",
    "            if (inter[k] >= thd) & (k != max_idx):\n",
    "                remove[k] = True\n",
    "\n",
    "    for k in range(0, len(rects)):\n",
    "        if not remove[k]:\n",
    "            out.append(rects[k])\n",
    "\n",
    "    boxes = [box[0] for box in out]\n",
    "    scores = [score[1] for score in out]\n",
    "    classes = [cls[2] for cls in out]\n",
    "    return boxes, scores, classes\n",
    "\n",
    "\n",
    "def intersection(rect1, rect2):\n",
    "    \"\"\"\n",
    "    Calculates square of intersection of two rectangles\n",
    "    rect: list with coords of top-right and left-boom corners [x1,y1,x2,y2]\n",
    "    return: square of intersection\n",
    "    \"\"\"\n",
    "    x_overlap = max(0, min(rect1[2], rect2[2]) - max(rect1[0], rect2[0]));\n",
    "    y_overlap = max(0, min(rect1[3], rect2[3]) - max(rect1[1], rect2[1]));\n",
    "    overlapArea = x_overlap * y_overlap;\n",
    "    return overlapArea\n",
    "\n",
    "\n",
    "def square(rect):\n",
    "    \"\"\"\n",
    "    Calculates square of rectangle\n",
    "    \"\"\"\n",
    "    return abs(rect[2] - rect[0]) * abs(rect[3] - rect[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'detection_boxes': [array([0.3460582 , 0.12145847, 0.97111255, 0.8437057 ], dtype=float32)],\n",
       " 'detection_scores': [0.69003004],\n",
       " 'detection_classes': [1]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path2images = ['3-1.jpg','4-3.jpg']\n",
    "inference_as_raw_output(path2images, box_th=0.4, path2dir=\"test_images\", to_file=True, data=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_with_plot(path2images, box_th=0.25):\n",
    "    \"\"\"\n",
    "    Function that performs inference and plots resulting b-boxes\n",
    "    \n",
    "    Args:\n",
    "      path2images: an array with pathes to images\n",
    "      box_th: (float) value that defines threshold for model prediction.\n",
    "      \n",
    "    Returns:\n",
    "      None\n",
    "    \"\"\"\n",
    "    for image_path in path2images:\n",
    "\n",
    "        print('Running inference for {}... '.format(image_path), end='')\n",
    "\n",
    "        image_np = load_image_into_numpy_array(image_path)\n",
    "        \n",
    "        input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "        detections = detect_fn(input_tensor)\n",
    "\n",
    "        # All outputs are batches tensors.\n",
    "        # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
    "        # We're only interested in the first num_detections.\n",
    "        num_detections = int(detections.pop('num_detections'))\n",
    "        detections = {key: value[0, :num_detections].numpy()\n",
    "                      for key, value in detections.items()}\n",
    "        \n",
    "        detections['num_detections'] = num_detections\n",
    "\n",
    "        # detection_classes should be ints.\n",
    "        detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "\n",
    "        label_id_offset = 1\n",
    "        image_np_with_detections = image_np.copy()\n",
    "\n",
    "        viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "                image_np_with_detections,\n",
    "                detections['detection_boxes'],\n",
    "                detections['detection_classes']+label_id_offset,\n",
    "                detections['detection_scores'],\n",
    "                category_index,\n",
    "                use_normalized_coordinates=True,\n",
    "                max_boxes_to_draw=200,\n",
    "                min_score_thresh=box_th,\n",
    "                agnostic_mode=False,\n",
    "                line_thickness=5)\n",
    "\n",
    "        plt.figure(figsize=(15,10))\n",
    "        plt.imshow(image_np_with_detections)\n",
    "        print('Done')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference for test_images/3-1.jpg... Done\n",
      "Running inference for test_images/4-3.jpg... Done\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "path2images = ['test_images/3-1.jpg', 'test_images/4-3.jpg']\n",
    "inference_with_plot(path2images, box_th=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_with_box(path2images, box_th=0.4):\n",
    "    res = []\n",
    "    for image_path in path2images:\n",
    "        image_np = load_image_into_numpy_array(image_path)\n",
    "        input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "        detections = detect_fn(input_tensor)\n",
    "\n",
    "        # All outputs are batches tensors.\n",
    "        # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
    "        # We're only interested in the first num_detections.\n",
    "        num_detections = int(detections.pop('num_detections'))\n",
    "        detections = {key: value[0, :num_detections].numpy()\n",
    "                      for key, value in detections.items()}\n",
    "        \n",
    "        detections['num_detections'] = num_detections\n",
    "\n",
    "        # detection_classes should be ints.\n",
    "        detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
    "\n",
    "        boxes = []\n",
    "        for i in range(len(detections['detection_boxes'])):\n",
    "            if detections['detection_scores'][i] > box_th:\n",
    "                boxes.append(detections['detection_boxes'][i])\n",
    "        res.append(boxes)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=90x57 at 0x1A738CDE7F0>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path2images = ['test_images/1-1.jpg']\n",
    "tmp = inference_with_box(path2images, box_th=0.4)\n",
    "image_np = load_image_into_numpy_array('test_images/1-1.jpg')\n",
    "\n",
    "a, b, c, d = tmp[0][1]\n",
    "xmin = int(np.round(b * 320))\n",
    "xmax = int(np.round(d * 320))\n",
    "ymin = int(np.round(a * 240))\n",
    "ymax = int(np.round(c * 240))\n",
    "xmin, ymin, xmax, ymax   # xmin,ymin is the top left corner; xmax,ymax is the bottom right corner\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import array_to_img\n",
    "test=array_to_img(image_np[ymin:ymax+1, xmin:xmax+1])\n",
    "\n",
    "test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "263c7e5886329c338e3eea93de1eefde229b9e56212206ac237d31476cf7523a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
